# 📦 Record Export with Streaming and SQL Cursor

This project is an efficient solution for exporting large volumes of data from PostgreSQL to CSV files using Node.js. The project leverages Node.js native streams and SQL cursors to perform data export in an optimized way, minimizing memory usage and improving performance compared to traditional approaches like using OFFSET.

Focused on handling large amounts of data, the project enables batch extraction and incremental writing of data to CSV, making it ideal for scenarios where the volume of records is too large to process all at once.

Key Features:
- Efficiency: Utilizes streaming and cursors to ensure low memory usage and high performance, even with large data sets.

- Scalability: The stream-based architecture allows the process to be scalable, efficiently handling large databases.

- Simple Integration: Easy integration with any Node.js application that uses PostgreSQL.

- Testable: The code is modular and tested, ensuring robustness and reliability in data export.

_If you need to export large volumes of data from PostgreSQL to CSV efficiently without overwhelming memory, this is the ideal project._

___

This project performs **efficient export of large amounts of records** from PostgreSQL to a `.csv` file using:
- [SQL Cursor](https://medium.com/@ietienam/efficient-pagination-with-postgresql-using-cursors-83e827148118) to avoid full in-memory loading
- Streaming with Node.js pipeline
- Incremental writing with `csv-stringify`

---

## 📋 Prerequisites

- [Node.js 23](https://nodejs.org/)
- [Docker](https://www.docker.com/)
- [pnpm](https://pnpm.io/) or `npm/yarn`

---

## 🚀 Setting up the environment with PostgreSQL

The project comes with a `docker-compose.yml` to easily set up the local database.

```bash
docker-compose up -d
```

The default connection will be:
```env
DATABASE_URL=postgres://postgres:postgres@localhost:5432/export
```

✅ Make sure the `DATABASE_URL` environment variable is properly configured (via .env.local or directly in the terminal)

---

## 🌱 Populating the Database with Test Data
Run the seed script to generate *1,000,000* records with random names and prices:

```bash
npm run db:seed
```

_If needed, you can change the number of items generated by modifying the values of the variables:_

```bash
const TOTAL_PRODUCTS = 10_000;
const MAX_PRODUCTS_CREATED = 100;
```

---

📤 Exporting the records to CSV

```bash
npm run export
```

--- 

🧪 Running the Tests
```bash
npm run test
```

--- 

## 📁 Project Structure
```bash
src/
├── db/
│   └── client.ts        # PostgreSQL connection
│   └── seed.ts          # Database population
├── export.ts            # Exporting with streams and csv-stringify
├── __tests__/
│   └── export.test.ts   # Unit tests for streaming and CSV
```

---

## 🚀 Technologies Used
- [Node.js](https://nodejs.org/docs/latest/api/synopsis.html) + [TypeScript](https://nodejs.org/en/learn/typescript/introduction)
- [PostgreSQL](https://www.postgresql.org/)
- [Docker](https://docs.docker.com/get-started/)
- [csv-stringify](https://www.npmjs.com/package/csv-stringify)
- [Jest](https://jestjs.io/docs/getting-started)
- [Streams (Readable, Transform, Writable)](https://nodejs.org/api/stream.html)
- [SQL Cursor with postgres.js](https://medium.com/@ietienam/efficient-pagination-with-postgresql-using-cursors-83e827148118)

---

## 📈 Benefits of the Architecture
✅ Efficient export with streaming  
✅ Low memory usage  
✅ Scalable performance with SQL cursor  
✅ Testable and modular code

---

## 📌 Notes
- The project is configured with `type`: `module` (ESM)
- The tsconfig.json is prepared for es2024 + NodeNext
- Using pipeline ensures error control and backpressure

---

📝 License

###### _This project is a refactor with improvements and adaptations of an open source project developed by Diego from Rocketseat. The original code can be found in [YouTube video](https://www.youtube.com/watch?v=TaYcpJQHJQE)._

MIT © 2025 — Iago Oliveira